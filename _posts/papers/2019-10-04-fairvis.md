---
layout: paper
categories: papers
permalink: papers/fairvis
id: fairvis
title: "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning"
authors: 
    - √Ångel Alexander Cabrera
    - Will Epperson 
    - Fred Hohman
    - Minsuk Kahng
    - Jamie Morgenstern 
    - Duen Horng (Polo) Chau
venue: IEEE Conference on Visual Analytics Science and Technology
venue-shorthand: VAST
location: Vancouver, Canada
year: 2019
url: /papers/fairvis
demo: https://poloclub.github.io/FairVis/
pdf: https://arxiv.org/abs/1904.05419
slides: /slides/19-fairvis-vast-slides.pdf
recording: https://vimeo.com/368702211
code: https://github.com/poloclub/FairVis
blog: https://medium.com/@cabreraalex/fairvis-discovering-bias-in-machine-learning-using-visual-analytics-acbd362a3e2f
selected: false
type: conference
figure: /images/papers/19-fairvis-vast.png
doi: "10.1109/VAST47406.2019.8986948"
feature-title: FairVis
feature-description: Visual analytics for discovering intersectional bias in machine learning
bibtex: |-

    @article{cabrera2019fairvis,
        title={FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning},
        author={Cabrera, {\'A}ngel and Epperson, Will, and Hohman, Fred and Kahng, Minsuk and Morgenstern, Jamie and Chau, Duen Horng},
        journal={IEEE Conference on Visual Analytics Science and Technology (VAST)},
        year={2019},
        publisher={IEEE},
        doi={10.1109/VAST47406.2019.8986948},
        url={https://poloclub.github.io/FairVis/}
    }
---

The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people.
Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups.
Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups.
We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models.
Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups.
FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups.
We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism.
As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.
