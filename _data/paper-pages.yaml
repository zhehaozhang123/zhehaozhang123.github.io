- id: chameleon
  figure: /images/papers/20-chameleon-chi.png
  caption: |
    The Chameleon interface integrates multiple coordinated views to help practitioners explore their evolving datasets.
    The <i>Data Version Timeline</i> (A) lists data versions across the top of the interface and allows users to select a primary and a secondary version to visualize below.
    The <i>Sidebar</i> (B) shows version summaries and multiples views that visualize changing instance predictions.
    Practitioners can use the sibebar views to filter data in the <i>Feature View</i> (C), which visualizes each feature of a dataset as a histogram with both selected data versions, faceted by performance and the train/testing split.
    Dataset and feature names are redacted for anonymity.
  abstract: |
    Successful machine learning (ML) applications require iterations on both modeling and the underlying data.
    While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models.
    We also identify common types of data iterations and associated analysis tasks and challenges.
    To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions.
    We present two case studies where developers apply Chameleon to their own evolving datasets on production ML projects.
    Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.

- id: notebook
  abstract: |
    A new kind of widget has begun appearing in the data science notebook programming community that can fluidly switch its own appearance between two representations: a graphical user interface (GUI) tool and plain textual code.
    Data scientists of all expertise levels routinely work in both visual GUIs (data visualizations or spreadsheets) and plaintext code (numerical, data manipulation, or machine learning libraries).
    These work tools have typically been separate.
    Here, we argue for the unique role and potential of fluid GUI/text programming to serve data work practices.
    We contribute a generalized method and API for robust fluid GUI/text coding in notebooks that addresses key questions in code generation and user-interactions.
    Finally, we demonstrate the potential of our method in two notebook tool examples and a usability study with professional data science and machine learning practitioners.


- id: cnn101
  figure: /images/papers/20-cnn101-arxiv.png
  caption: |
    The CNN 101 user interface and its tightly coupled, multiple views.
    The Overview (A) visualizes activation maps of all neurons as heatmaps connected with edges.
    When user clicks a convolutional neuron in (A), the view transitions to the Convolutional Intermediate View (A) -> (B).
    The Flatten Intermediate View appears when an output neuron is selected instead (A) -> (C).
    (B) demonstrates the relationship between selected convolutional neuron and its previous layer.
    (B) transitions to the Detail View which illustrates the convolution operation on selected input neuron (B) -> (D).
    (C) explains the flatten layer between the second last layer and output layer.
  abstract: |
    The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology.
    However, it is often challenging for learners to take the first steps due to the complexity of deep learning models.
    We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks.
    Through tightly integrated interactive views,
    CNN 101 offers both overview and detailed descriptions of how a model works.
    Built using modern web technologies, CNN 101 runs locally in users' web browsers without requiring specialized hardware, broadening the public's education access to modern deep learning techniques.

- id: massif
  figure: /images/papers/20-massif-arxiv.png
  caption: |
      The Massif interface.
      A user Hailey is studying a targeted version of the Fast Gradient Method (FGM) attack performed on the InceptionV1 neural network model. 
      Using the control panel (A), she selects "giant panda" as the benign class and "armadillo" as the attack target class.
      Massif generates an attribution graph of the model (B), which shows Hailey the neurons within the network that are suppressed in the attacked images (B1, colored blue on the left), shared by both benign and attacked images (B2, colored purple in the middle), and emphasized only in the attacked images (B3, colored red on the right). 
      Each neuron is represented by a node and its feature visualization (C).
      Hovering over any neuron displays example dataset patches that maximally activate the neuron, providing stronger evidence for what a neuron has learned to detect. 
      Hovering over a neuron also highlights its most influential connections from the previous layer (D), allowing Hailey to determine where in the network the prediction diverges from the benign class to the attacked class.
  abstract: |
    Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as "black boxes" in such applications.
    Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world.
    To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks.
    Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack.
    Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. 
    These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.

- id: summit
  figure: /images/papers/19-summit-vast.png
  caption: |
    With Summit, users can scalably summarize and interactively interpret deep neural networks by visualizing <i>what</i> features a network detects and <i>how</i> they are related.
    In this example, InceptionV1 accurately classifies images of <i>tench</i> (yellow-brown fish).
    However, Summit reveals surprising associations in the network (e.g., using parts of people) that contribute to its final outcome: the "tench" prediction is dependent on an intermediate "hands holding fish" feature (right callout), which is influenced by lower-level features like <i>"scales,"</i> <i>"person,"</i> and <i>"fish"</i>. 
    A. The Embedding View summarizes all classes' aggregated activations using dimensionality reduction.
    B. The Class Sidebar enables users to search, sort, and compare all classes within a model.
    C. The Attribution Graph View visualizes highly activated neurons as vertices ("scales" "fish") and their most influential connections as edges (dashed purple edges).
  abstract: |
    Deep learning is increasingly used in decision-making tasks.
    However, understanding how neural networks produce final predictions remains a fundamental challenge.
    Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons.
    As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture.
    We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions.
    Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. 
    Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes.
    Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations.
    We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design.
    The Summit visualization runs in modern web browsers and is open-sourced.

- id: fairvis
  figure: /images/papers/19-fairvis-vast.png
  caption: |
    FairVis integrates multiple coordinated views for discovering intersectional bias. 
    Above, our user investigates the intersectional subgroups of <i>sex</i> and <i>race</i>. 
    A. The Feature Distribution View allows users to visualize each feature's distribution and generate subgroups.
    B. The Subgroup Overview lets users select various fairness metrics to see the global average per metric and compare subgroups to one another, e.g., pinned Caucasian Males versus hovered African-American Males.
    The plots for <i>Recall</i> and <i>False Positive Rate</i> show that for African-American Males, the model has relatively high recall but also the highest false positive rate out of all subgroups of sex and race. 
    C. The Detailed Comparison View lets users compare the details of two groups and investigate their class balances.
    Since the difference in False Positive Rates between Caucasian Males and African-American Males is far larger than their difference in base rates, a user suspects this part of the model merits further inquiry. 
    D. The Suggested and Similar Subgroup View shows suggested subgroups ranked by the worst performance in a given metric. 
  abstract: |
    The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people.
    Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups.
    Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups.
    We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models.
    Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups.
    FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups.
    We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism.
    As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.


- id: telegam
  figure: /images/papers/19-telegam-vis.png
  caption: |
    The TeleGam user interface contains (A) a model selector and parameters for the visualizations and verbalizations.
    (B) The Global Model View displays model feature-level verbalizations of GAM shape function charts that describe a feature's overall impact on model predictions.
    (C) The Local Instance View displays two data instance's waterfall charts, an explanatory visualization that shows the cumulative sum of the contribution each feature has on the final prediction.
    Alongside are instance-level verbalizations that, when brushed, highlight in orange the corresponding marks of the visualization that the verbalization refers to.
    (D) Settings to interactively tune verbalization generation thresholds.
    
  abstract: |
    While machine learning (ML) continues to find success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging.
    Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models.
    However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difficult and may require additional expertise.
    Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances.
    With the potential benefits of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. 
    Specifically, we present a prototype system, TeleGam, that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs).
    We describe TeleGam's interface and underlying heuristics to generate the verbalizations.
    We conclude by discussing how TeleGam can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.

- id: electrolens
  figure: /images/papers/19-electrolens-vis.png
  caption: |
    The ElectroLens user interface (UI) visualizing the electron cloud of CH_3NO_2 molecule. 
    The UI has two main parts: 3D viewer(s) on the left, and 2D plots on the right with option boxes in each view for customization. 
    (a) The 3D viewer for Cartesian space.
    ElectroLens uses a "point cloud" to mimic the electron cloud, where the density of points corresponds to the density of electron cloud, and the color corresponds to energy density in this case.
    ElectroLens utilizes the ball-and-stick model to visualize atoms, where different atom types are denoted by white (hydrogen), grey (carbon), blue (nitrogen), and red (oxygen) spheres.
    (B) 2D plots for exploring and plotting additional features.
    The upper plot is the correlation plot used to identify the potentially interesting combinations of features.
    The plot below is a scatter plot of two features (in log-scale), where the color codes the number of data points represented.
    (C) Selecting parts on the scatter plot causes the corresponding points in 3D viewer and other scatter plots to be highlighted.
    This case illustrates the connection between the features (right) and the chemical concept of a C-N bond (left).
    
  abstract: |
    In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory.
    These techniques often require researchers to engineer abstract "features" that encode chemical concepts into a mathematical form compatible with the input to machine-learning models.
    However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features.
    We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem.
    The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots.
    The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection.
    It is built to be scalable, and integrate with existing infrastructure.

- id: gamut
  figure: /images/papers/19-gamut-chi.png
  caption: |
    Interacting with Gamut's multiple coordinated views together.
    (A) Selecting the OverallQual feature from the sorted Feature Sidebar displays its shape curve in the Shape Curve View.
    (B) Brushing over either explanation for Instance 550 or Instance 798 shows the contribution of the Ove
  abstract: |
    Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations.
    This has led to a rallying cry for model interpretability.
    Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. 
    Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation.
    Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability.
    Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness.
    Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.

- id: subgroup
  figure: /images/papers/19-subgroup-debugml.png
  caption: |
    Using our technique on the UCI Adult Dataset we (A) Cluster instances into subgroups, then (B) Calculate subgroup feature entropy to find dominant features, and lastly (C) Investigate similar subgroups to discover value and performance differences.
  abstract: |
    As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups.
    Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes.
    These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features.
    We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms.
    Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias.
    Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.

- id: atlas
  figure: /images/papers/19-atlas-iui.png
  caption: |
    Atlas adapts scalable edge decomposition to provide novel modes of large graph exploration, through three coordinated views.
    A. Our user Don first explores the edge decomposition of a word embedding graph in the Overview by decomposing a graph into 3D graph layers.
    B. Don then inspects the Ribbon for a summary of the layers.
    C. From the word "dismayed," in layer 8, Don performs cross-layer exploration, to reach layer 5.
    Using the Layer view's interactive node-link diagrams, Don discovers a component in the word embedding describing one's surprise, where neutral words (e.g., "surpised" and "surprising") bridge multiple quasi-cliques that describe more positive (e.g., "remarkable" and "astounding") and negative (e.g., "irked" and "incensed") surprise words.
    Blue perspective planes, and red and green ellipses are illustrative annotations.
  abstract: |
    Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking.
    To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations.
    Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns.
    Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges.
    We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings.
    Atlas runs in-browser and is open-sourced. 

- id: kcore
  abstract: |
    We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs.
    Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds.
    Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition's layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.

- id: compression
  figure: /images/papers/18-compression-kdd.png
  caption: |
    Screenshot of Adagio with an example usage scenario.
    (1) Jane uploads an audio file that is transcribed by DeepSpeech (an ASR model); then she performs an adversarial attack on the audio in real time by entering a target transcription after selecting the attack option from the dropdown menu.
    (2) Jane decides to perturb the audio to change the last word of the sentence from "joanna" to "marissa"; she can listen to the original audio and see the transcription by clicking on the "Original" badge.
    (3) Jane applies MP3 compression to recover the original, correct transcription from the manipulated audio;
    clicking on a waveform plays back the  audio from the selected position.
    (4) Jane can experiment with multiple audio samples by adding more cards.
  abstract: |
    Research in the upcoming field of adversarial ML has revealed that machine learning, especially deep learning, is highly vulnerable to imperceptible adversarial perturbations, both in the domain of vision as well as speech.
    This has induced an urgent need to devise fast and practical approaches to secure deep learning models from adversarial attacks, so that they can be safely deployed in real-world applications. 
    In this showcase, we put forth the idea of compression as a viable solution to defend against adversarial attacks across modalities. 
    Since most of these attacks depend on the gradient of the model to craft an adversarial instance, compression, which is usually non-differentiable, denies a useful gradient to the attacker. 
    In the vision domain we have JPEG compression, and in the audio domain we have MP3 compression and AMR encoding -- all widely adopted techniques that have very fast implementations on most platforms, and can be feasibly leveraged as defenses. 
    We will show the effectiveness of these techniques against adversarial attacks through live demonstrations, both for vision as well as speech.
    These demonstrations would include real-time computation of adversarial perturbations for images and audio, as well as interactive application of compression for defense. 
    We would invite and encourage the audience to experiment with their own images and audio samples during the demonstrations. 
    This work was undertaken jointly by researchers from Georgia Institute of Technology and Intel Corporation.

- id: shield
  figure: /images/papers/18-shield-kdd.png
  caption: |
    Shield Framework Overview. Shield combats adversarial images (in red), by removing perturbation in real time using Stochastic Local Quantization (SLQ) and an ensemble of vaccinated models robust to compression transformation for both adversarial and benign images.
    Our approach eliminates up to 94% of black-box attacks and 98% of gray-box attacks delivered by some of the most recent, strongest attacks, such as Carlini-Wagner’s L2 and DeepFool.
  abstract: |
    The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images.
    This underscores the urgent need for practical defense that can be readily deployed to combat attacks in real-time.
    Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed Shield defense framework, utilizing its capability to effectively "compress away" such pixel manipulation.
    To immunize a DNN model from artifacts introduced by compression, Shield "vaccinates" a model by re-training it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense.
    On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed.
    This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged protection.
    We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 94% of black-box attacks and 98% of gray-box attacks delivered by the recent, strongest attacks, such as Carlini-Wagner's L2 and DeepFool.
    Our approaches are fast and work without requiring knowledge about the model.

- id: graph-playground
  figure: /images/papers/18-playground-arxiv.png
  caption: |
    The Graph Playground user interface.
    Graph Playground is composed of three main views: the 3D Overview (left), the Graph Ribbon (middle), and the Layers view (right).
    The Graph Ribbon that splits the display can be dragged left and right to adjust the visible screen real estate that either the Overview or Layers view shows.
    In the figure, the vertex "caeciliidae" is selected, coloring it blue in both the Overview and Layers view.
    Here we see "caeciliidae" (a worm-like amphibian) in layer 30 bridges two quasi-cliques (families of birds and families of sea snails) together, while its clone in layer 25 participates in another single quasi-clique (families of land creatures).
  abstract: |
    We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs.
    Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds.
    Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition's layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.

- id: ie
  figure: /images/papers/18-interactive-cvpr.png
  caption: |
    The modified image (left), originally classified as <i>dock</i> is misclassified as <i>ocean liner</i> when the masts of a couple boats are removed from the original image (right).
    The top five classification scores are tabulated underneath each image.
  abstract: |
    We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. 
    Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time.
    This system allows users to compare and contrast what image regions humans and machine learning models use for classification.

- id: vigor
  figure: /images/papers/17-vigor-vast.png
  caption: |
    A screenshot of VIGOR showing an analyst exploring a DBLP co-authorship network, looking for researchers who have co-authored papers at the VAST and KDD conferences.
    (A) The Exemplar View visualizes the query, and (B) the Fusion Graph shows the induced graph formed by joining all query matches. 
    Picking constant node values (e.g., Shixia) in the Exemplar View filters the Fusion Graph.
    (C) Hovering over a node shows its details.
    (D) The Subgraph Embedding embeds each match as a point in lower-dimensional space and clusters them to allow analysts to see patterns and outliers.
    (E) The Feature Explorer summarizes each cluster’s feature distributions.
  abstract: |
    Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners).
    While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results.
    Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries.
    We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results.
    VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process.
    VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization.
    Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents.
    We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.

- id: playground
  figure: /images/papers/17-playground-vast.png
  caption: |
    (A) The Les Miserables co-occurrence network of the novel’s characters, visualized using standard force-directed layout.
    (B) A graph playground created by applying fixed-point edge decomposition, producing cloned vertices that appear in multiple layers.
    The character Valjean appears in six layers; his clones are connected using a vertical black line, and his egonet is highlighted in every layer.
    (C) The graph playground layers separated and individually redrawn using force-directed layout in 2D, with Valjean’s colored egonet still shown. Our method reveals interesting subgraph structures and distributes them into layers, e.g., stars in layer 1 (blue), and a clique in layer 6 (brown).
    Valjean’s vertex is colored black in every layer he exists in (all layers except layer 9), highlighting his central role in the novel and his diverse participation in different graph patterns.
  abstract: |
    We use an iterative edge decomposition approach, derived from the popular iterative vertex peeling strategy, to globally split each vertex egonet (subgraph induced by a vertex and its neighbors) into a collection of edge-disjoint layers. 
    Each layer is an edge maximal induced subgraph of minimum degree k that determines the layer density.
    This edge decomposition is derived completely from the overall network topology, and since each vertex can appear in multiple layers, we can associate to each vertex a vector profile that can be used to identify its different "roles" across the network.
    This allows us to explore a network’s topology at different levels of granularity, e.g., per layer and across layers.
    This is only feasible by mapping simultaneously a vertex to a set of 3D coordinates (x, y, and z) where the third coordinate encodes the different layers a vertex belongs to.
    This is one of the few instances where 3D visualization enhances graph exploration and navigation in an arguably "natural" way: a graph now becomes a 3D graph playground where a vertex plays a certain "role" per layer that is determined by the overall network topology.
    Our approach helps disentangle "hairball" looking embeddings produced by conventional 2D graph drawings.

- id: got
  figure: /images/papers/17-got-vis4dh.png
  caption: |
    The interactive visualization, displaying the show's Season 2, Episode 9: "Blackwater."
    The Color Representation plot (A) shows the palettes extracted for the entire series, each season, and each episode as labelled stacked columns, e.g. S2E9 corresponds to Season 2, Episode 9.
    The dialogue plot (B) shows how much of each textual category is present in the dialogue at a given time.
    Notice that the bottom color plot in (A) shares the horizontal time axis with the bubbles directly below in (B).
    Metadata for each season and episode is displayed in (C).
    The frames view (D) and dialogue view (E) display the video frames and spoken dialogue for the current time slice and update as users interact with the data in (A) and (B).
    The word frequency histogram (F) displays the top words spoken in the episode.
  abstract: |
    Films and television shows provide a rich source of cultural data and form an integral part of modern life. 
    However, the video medium remains difficult to analyze at scale effectively and its study has generally attracted limited research attention. 
    We propose a method of summarizing the audio and visual aspects of entertainment videos, through the automatic extraction of dominant colors from video frames and textual categories from dialogue. 
    The colors and dialogue are displayed in a visualization that allows the user to explore the video, highlighting both high-level and low-level patterns from the data. 
    Focusing on the hit television series *Game of Thrones*, we show how our visualization supports the detection of scene changes and plot points, providing a new perspective for both scholars and casual viewers.

- id: mhealth
  figure: /images/papers/17-dashboard-ubicomp.png
  caption: |
    The Discovery Dashboard interface showing data from a mobile sensor study. Each row corresponds to one participant's data. 
    A user-defined motif (for participant 6012) is selected, and the system automatically finds similar motifs across all participants and highlights them in yellow.
    This particular motif is a recurring pattern for participant 6012, often found near smoking lapses (vertical red dotted lines).
  abstract: |
    We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. 
    Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data.
    Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do --- in the web-browser and in real time.
    We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.

- id: jpeg
  figure: /images/papers/17-defense-arxiv.png
  caption: |
    A comparison of the classification results of an exemplar image from the German Traffic Sign Recognition Benchmark (GTSRB) dataset.
    A benign image (left) is originally classified as a <i>stop sign</i>, but after the addition of an adversarial perturbation to the image (middle) the resulting image is classified as a <i>max speed 100</i> sign.
    Using JPEG compression on the adversarial image (right), we recover the original classification of <i>stop sign</i>.
  abstract: |
    Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. 
    However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. 
    These adversarial samples are crafted by adding small perturbations to normal, benign images. 
    Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. 
    In this work, we explore and demonstrate 
    how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to  counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). 
    An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. 
    Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations.
    Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.

- id: visage
  figure: /images/papers/17-visage-sigmod.png
  caption: |
    The interface for our demonstration shows a basic query for a film with at least one actor and one director; results are shown on the right in real time.
    Queries are constructed in the open space (1) by placing nodes and edges.
    The query results are shown in a list in (2). When a node-result is clicked, a summary of feature conditions (2.1) is shown with a summary of that node’s attributes (2.2).
    In this example the film must have a critics’ score of “Well-rated”.
  abstract: |
    Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages.
    We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction.
    We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.

- id: shapeshop
  figure: /images/papers/17-shapeshop-chi.png
  caption: |
    The ShapeShop system user interface is divided into two main sections.
    The Model Builder (top) contains the training data, model, and hyperparameter selection options where a user follows enumerated steps, concluding with the system building and training an N-image classifier, where each training image selected corresponds to one class.
    In the Experiment Results section (bottom), each time the "Train and Visualize" button is clicked a new set of results appears including the class activation maximization of each class.
  abstract: |
    Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as "black-boxes" due to their internal complexity that is hard to understand.
    Little research focuses on helping people explore and understand the relationship between a user's data and the learned representations in deep learning models.
    We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned.
    Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.   
